{"ast":null,"code":"// The implementation of the [HTTP/2 Header Compression][http2-compression] spec is separated from\n// the 'integration' part which handles HEADERS and PUSH_PROMISE frames. The compression itself is\n// implemented in the first part of the file, and consists of three classes: `HeaderTable`,\n// `HeaderSetDecompressor` and `HeaderSetCompressor`. The two latter classes are\n// [Transform Stream][node-transform] subclasses that operate in [object mode][node-objectmode].\n// These transform chunks of binary data into `[name, value]` pairs and vice versa, and store their\n// state in `HeaderTable` instances.\n//\n// The 'integration' part is also implemented by two [Transform Stream][node-transform] subclasses\n// that operate in [object mode][node-objectmode]: the `Compressor` and the `Decompressor`. These\n// provide a layer between the [framer](framer.html) and the\n// [connection handling component](connection.html).\n//\n// [node-transform]: https://nodejs.org/api/stream.html#stream_class_stream_transform\n// [node-objectmode]: https://nodejs.org/api/stream.html#stream_new_stream_readable_options\n// [http2-compression]: https://tools.ietf.org/html/rfc7541\nexports.HeaderTable = HeaderTable;\nexports.HuffmanTable = HuffmanTable;\nexports.HeaderSetCompressor = HeaderSetCompressor;\nexports.HeaderSetDecompressor = HeaderSetDecompressor;\nexports.Compressor = Compressor;\nexports.Decompressor = Decompressor;\n\nvar TransformStream = require('stream').Transform;\n\nvar assert = require('assert');\n\nvar util = require('util'); // Header compression\n// ==================\n// The HeaderTable class\n// ---------------------\n// The [Header Table] is a component used to associate headers to index values. It is basically an\n// ordered list of `[name, value]` pairs, so it's implemented as a subclass of `Array`.\n// In this implementation, the Header Table and the [Static Table] are handled as a single table.\n// [Header Table]: https://tools.ietf.org/html/rfc7541#section-2.3.2\n// [Static Table]: https://tools.ietf.org/html/rfc7541#section-2.3.1\n\n\nfunction HeaderTable(log, limit) {\n  var self = HeaderTable.staticTable.map(entryFromPair);\n  self._log = log;\n  self._limit = limit || DEFAULT_HEADER_TABLE_LIMIT;\n  self._staticLength = self.length;\n  self._size = 0;\n  self._enforceLimit = HeaderTable.prototype._enforceLimit;\n  self.add = HeaderTable.prototype.add;\n  self.setSizeLimit = HeaderTable.prototype.setSizeLimit;\n  return self;\n}\n\nfunction entryFromPair(pair) {\n  var entry = pair.slice();\n  entry._size = size(entry);\n  return entry;\n} // The encoder decides how to update the header table and as such can control how much memory is\n// used by the header table.  To limit the memory requirements on the decoder side, the header table\n// size is bounded.\n//\n// * The default header table size limit is 4096 bytes.\n// * The size of an entry is defined as follows: the size of an entry is the sum of its name's\n//   length in bytes, of its value's length in bytes and of 32 bytes.\n// * The size of a header table is the sum of the size of its entries.\n\n\nvar DEFAULT_HEADER_TABLE_LIMIT = 4096;\n\nfunction size(entry) {\n  return new Buffer(entry[0] + entry[1], 'utf8').length + 32;\n} // The `add(index, entry)` can be used to [manage the header table][tablemgmt]:\n// [tablemgmt]: https://tools.ietf.org/html/rfc7541#section-4\n//\n// * it pushes the new `entry` at the beggining of the table\n// * before doing such a modification, it has to be ensured that the header table size will stay\n//   lower than or equal to the header table size limit. To achieve this, entries are evicted from\n//   the end of the header table until the size of the header table is less than or equal to\n//   `(this._limit - entry.size)`, or until the table is empty.\n//\n//              <----------  Index Address Space ---------->\n//              <-- Static  Table -->  <-- Header  Table -->\n//              +---+-----------+---+  +---+-----------+---+\n//              | 0 |    ...    | k |  |k+1|    ...    | n |\n//              +---+-----------+---+  +---+-----------+---+\n//                                     ^                   |\n//                                     |                   V\n//                              Insertion Point       Drop Point\n\n\nHeaderTable.prototype._enforceLimit = function _enforceLimit(limit) {\n  var droppedEntries = [];\n\n  while (this._size > 0 && this._size > limit) {\n    var dropped = this.pop();\n    this._size -= dropped._size;\n    droppedEntries.unshift(dropped);\n  }\n\n  return droppedEntries;\n};\n\nHeaderTable.prototype.add = function (entry) {\n  var limit = this._limit - entry._size;\n\n  var droppedEntries = this._enforceLimit(limit);\n\n  if (this._size <= limit) {\n    this.splice(this._staticLength, 0, entry);\n    this._size += entry._size;\n  }\n\n  return droppedEntries;\n}; // The table size limit can be changed externally. In this case, the same eviction algorithm is used\n\n\nHeaderTable.prototype.setSizeLimit = function setSizeLimit(limit) {\n  this._limit = limit;\n\n  this._enforceLimit(this._limit);\n}; // [The Static Table](https://tools.ietf.org/html/rfc7541#section-2.3.1)\n// ------------------\n// The table is generated with feeding the table from the spec to the following sed command:\n//\n//     sed -re \"s/\\s*\\| [0-9]+\\s*\\| ([^ ]*)/  [ '\\1'/g\" -e \"s/\\|\\s([^ ]*)/, '\\1'/g\" -e 's/ \\|/],/g'\n\n\nHeaderTable.staticTable = [[':authority', ''], [':method', 'GET'], [':method', 'POST'], [':path', '/'], [':path', '/index.html'], [':scheme', 'http'], [':scheme', 'https'], [':status', '200'], [':status', '204'], [':status', '206'], [':status', '304'], [':status', '400'], [':status', '404'], [':status', '500'], ['accept-charset', ''], ['accept-encoding', 'gzip, deflate'], ['accept-language', ''], ['accept-ranges', ''], ['accept', ''], ['access-control-allow-origin', ''], ['age', ''], ['allow', ''], ['authorization', ''], ['cache-control', ''], ['content-disposition', ''], ['content-encoding', ''], ['content-language', ''], ['content-length', ''], ['content-location', ''], ['content-range', ''], ['content-type', ''], ['cookie', ''], ['date', ''], ['etag', ''], ['expect', ''], ['expires', ''], ['from', ''], ['host', ''], ['if-match', ''], ['if-modified-since', ''], ['if-none-match', ''], ['if-range', ''], ['if-unmodified-since', ''], ['last-modified', ''], ['link', ''], ['location', ''], ['max-forwards', ''], ['proxy-authenticate', ''], ['proxy-authorization', ''], ['range', ''], ['referer', ''], ['refresh', ''], ['retry-after', ''], ['server', ''], ['set-cookie', ''], ['strict-transport-security', ''], ['transfer-encoding', ''], ['user-agent', ''], ['vary', ''], ['via', ''], ['www-authenticate', '']]; // The HeaderSetDecompressor class\n// -------------------------------\n// A `HeaderSetDecompressor` instance is a transform stream that can be used to *decompress a\n// single header set*. Its input is a stream of binary data chunks and its output is a stream of\n// `[name, value]` pairs.\n//\n// Currently, it is not a proper streaming decompressor implementation, since it buffer its input\n// until the end os the stream, and then processes the whole header block at once.\n\nutil.inherits(HeaderSetDecompressor, TransformStream);\n\nfunction HeaderSetDecompressor(log, table) {\n  TransformStream.call(this, {\n    objectMode: true\n  });\n  this._log = log.child({\n    component: 'compressor'\n  });\n  this._table = table;\n  this._chunks = [];\n} // `_transform` is the implementation of the [corresponding virtual function][_transform] of the\n// TransformStream class. It collects the data chunks for later processing.\n// [_transform]: https://nodejs.org/api/stream.html#stream_transform_transform_chunk_encoding_callback\n\n\nHeaderSetDecompressor.prototype._transform = function _transform(chunk, encoding, callback) {\n  this._chunks.push(chunk);\n\n  callback();\n}; // `execute(rep)` executes the given [header representation][representation].\n// [representation]: https://tools.ietf.org/html/rfc7541#section-6\n// The *JavaScript object representation* of a header representation:\n//\n//     {\n//       name: String || Integer,  // string literal or index\n//       value: String || Integer, // string literal or index\n//       index: Boolean            // with or without indexing\n//     }\n//\n// *Important:* to ease the indexing of the header table, indexes start at 0 instead of 1.\n//\n// Examples:\n//\n//     Indexed:\n//     { name: 2  , value: 2  , index: false }\n//     Literal:\n//     { name: 2  , value: 'X', index: false } // without indexing\n//     { name: 2  , value: 'Y', index: true  } // with indexing\n//     { name: 'A', value: 'Z', index: true  } // with indexing, literal name\n\n\nHeaderSetDecompressor.prototype._execute = function _execute(rep) {\n  this._log.trace({\n    key: rep.name,\n    value: rep.value,\n    index: rep.index\n  }, 'Executing header representation');\n\n  var entry, pair;\n\n  if (rep.contextUpdate) {\n    this._table.setSizeLimit(rep.newMaxSize);\n  } // * An _indexed representation_ entails the following actions:\n  //   * The header field corresponding to the referenced entry is emitted\n  else if (typeof rep.value === 'number') {\n      var index = rep.value;\n      entry = this._table[index];\n      pair = entry.slice();\n      this.push(pair);\n    } // * A _literal representation_ that is _not added_ to the header table entails the following\n    //   action:\n    //   * The header is emitted.\n    // * A _literal representation_ that is _added_ to the header table entails the following further\n    //   actions:\n    //   * The header is added to the header table.\n    //   * The header is emitted.\n    else {\n        if (typeof rep.name === 'number') {\n          pair = [this._table[rep.name][0], rep.value];\n        } else {\n          pair = [rep.name, rep.value];\n        }\n\n        if (rep.index) {\n          entry = entryFromPair(pair);\n\n          this._table.add(entry);\n        }\n\n        this.push(pair);\n      }\n}; // `_flush` is the implementation of the [corresponding virtual function][_flush] of the\n// TransformStream class. The whole decompressing process is done in `_flush`. It gets called when\n// the input stream is over.\n// [_flush]: https://nodejs.org/api/stream.html#stream_transform_flush_callback\n\n\nHeaderSetDecompressor.prototype._flush = function _flush(callback) {\n  var buffer = concat(this._chunks); // * processes the header representations\n\n  buffer.cursor = 0;\n\n  while (buffer.cursor < buffer.length) {\n    this._execute(HeaderSetDecompressor.header(buffer));\n  }\n\n  callback();\n}; // The HeaderSetCompressor class\n// -----------------------------\n// A `HeaderSetCompressor` instance is a transform stream that can be used to *compress a single\n// header set*. Its input is a stream of `[name, value]` pairs and its output is a stream of\n// binary data chunks.\n//\n// It is a real streaming compressor, since it does not wait until the header set is complete.\n//\n// The compression algorithm is (intentionally) not specified by the spec. Therefore, the current\n// compression algorithm can probably be improved in the future.\n\n\nutil.inherits(HeaderSetCompressor, TransformStream);\n\nfunction HeaderSetCompressor(log, table) {\n  TransformStream.call(this, {\n    objectMode: true\n  });\n  this._log = log.child({\n    component: 'compressor'\n  });\n  this._table = table;\n  this.push = TransformStream.prototype.push.bind(this);\n}\n\nHeaderSetCompressor.prototype.send = function send(rep) {\n  this._log.trace({\n    key: rep.name,\n    value: rep.value,\n    index: rep.index\n  }, 'Emitting header representation');\n\n  if (!rep.chunks) {\n    rep.chunks = HeaderSetCompressor.header(rep);\n  }\n\n  rep.chunks.forEach(this.push);\n}; // `_transform` is the implementation of the [corresponding virtual function][_transform] of the\n// TransformStream class. It processes the input headers one by one:\n// [_transform]: https://nodejs.org/api/stream.html#stream_transform_transform_chunk_encoding_callback\n\n\nHeaderSetCompressor.prototype._transform = function _transform(pair, encoding, callback) {\n  var name = pair[0].toLowerCase();\n  var value = pair[1];\n  var entry, rep; // * tries to find full (name, value) or name match in the header table\n\n  var nameMatch = -1,\n      fullMatch = -1;\n\n  for (var droppedIndex = 0; droppedIndex < this._table.length; droppedIndex++) {\n    entry = this._table[droppedIndex];\n\n    if (entry[0] === name) {\n      if (entry[1] === value) {\n        fullMatch = droppedIndex;\n        break;\n      } else if (nameMatch === -1) {\n        nameMatch = droppedIndex;\n      }\n    }\n  }\n\n  var mustNeverIndex = name === 'cookie' && value.length < 20 || name === 'set-cookie' && value.length < 20 || name === 'authorization';\n\n  if (fullMatch !== -1 && !mustNeverIndex) {\n    this.send({\n      name: fullMatch,\n      value: fullMatch,\n      index: false\n    });\n  } // * otherwise, it will be a literal representation (with a name index if there's a name match)\n  else {\n      entry = entryFromPair(pair);\n      var indexing = entry._size < this._table._limit / 2 && !mustNeverIndex;\n\n      if (indexing) {\n        this._table.add(entry);\n      }\n\n      this.send({\n        name: nameMatch !== -1 ? nameMatch : name,\n        value: value,\n        index: indexing,\n        mustNeverIndex: mustNeverIndex,\n        contextUpdate: false\n      });\n    }\n\n  callback();\n}; // `_flush` is the implementation of the [corresponding virtual function][_flush] of the\n// TransformStream class. It gets called when there's no more header to compress. The final step:\n// [_flush]: https://nodejs.org/api/stream.html#stream_transform_flush_callback\n\n\nHeaderSetCompressor.prototype._flush = function _flush(callback) {\n  callback();\n}; // [Detailed Format](https://tools.ietf.org/html/rfc7541#section-5)\n// -----------------\n// ### Integer representation ###\n//\n// The algorithm to represent an integer I is as follows:\n//\n// 1. If I < 2^N - 1, encode I on N bits\n// 2. Else, encode 2^N - 1 on N bits and do the following steps:\n//    1. Set I to (I - (2^N - 1)) and Q to 1\n//    2. While Q > 0\n//       1. Compute Q and R, quotient and remainder of I divided by 2^7\n//       2. If Q is strictly greater than 0, write one 1 bit; otherwise, write one 0 bit\n//       3. Encode R on the next 7 bits\n//       4. I = Q\n\n\nHeaderSetCompressor.integer = function writeInteger(I, N) {\n  var limit = Math.pow(2, N) - 1;\n\n  if (I < limit) {\n    return [new Buffer([I])];\n  }\n\n  var bytes = [];\n\n  if (N !== 0) {\n    bytes.push(limit);\n  }\n\n  I -= limit;\n  var Q = 1,\n      R;\n\n  while (Q > 0) {\n    Q = Math.floor(I / 128);\n    R = I % 128;\n\n    if (Q > 0) {\n      R += 128;\n    }\n\n    bytes.push(R);\n    I = Q;\n  }\n\n  return [new Buffer(bytes)];\n}; // The inverse algorithm:\n//\n// 1. Set I to the number coded on the lower N bits of the first byte\n// 2. If I is smaller than 2^N - 1 then return I\n// 2. Else the number is encoded on more than one byte, so do the following steps:\n//    1. Set M to 0\n//    2. While returning with I\n//       1. Let B be the next byte (the first byte if N is 0)\n//       2. Read out the lower 7 bits of B and multiply it with 2^M\n//       3. Increase I with this number\n//       4. Increase M by 7\n//       5. Return I if the most significant bit of B is 0\n\n\nHeaderSetDecompressor.integer = function readInteger(buffer, N) {\n  var limit = Math.pow(2, N) - 1;\n  var I = buffer[buffer.cursor] & limit;\n\n  if (N !== 0) {\n    buffer.cursor += 1;\n  }\n\n  if (I === limit) {\n    var M = 0;\n\n    do {\n      I += (buffer[buffer.cursor] & 127) << M;\n      M += 7;\n      buffer.cursor += 1;\n    } while (buffer[buffer.cursor - 1] & 128);\n  }\n\n  return I;\n}; // ### Huffman Encoding ###\n\n\nfunction HuffmanTable(table) {\n  function createTree(codes, position) {\n    if (codes.length === 1) {\n      return [table.indexOf(codes[0])];\n    } else {\n      position = position || 0;\n      var zero = [];\n      var one = [];\n\n      for (var i = 0; i < codes.length; i++) {\n        var string = codes[i];\n\n        if (string[position] === '0') {\n          zero.push(string);\n        } else {\n          one.push(string);\n        }\n      }\n\n      return [createTree(zero, position + 1), createTree(one, position + 1)];\n    }\n  }\n\n  this.tree = createTree(table);\n  this.codes = table.map(function (bits) {\n    return parseInt(bits, 2);\n  });\n  this.lengths = table.map(function (bits) {\n    return bits.length;\n  });\n}\n\nHuffmanTable.prototype.encode = function encode(buffer) {\n  var result = [];\n  var space = 8;\n\n  function add(data) {\n    if (space === 8) {\n      result.push(data);\n    } else {\n      result[result.length - 1] |= data;\n    }\n  }\n\n  for (var i = 0; i < buffer.length; i++) {\n    var byte = buffer[i];\n    var code = this.codes[byte];\n    var length = this.lengths[byte];\n\n    while (length !== 0) {\n      if (space >= length) {\n        add(code << space - length);\n        code = 0;\n        space -= length;\n        length = 0;\n      } else {\n        var shift = length - space;\n        var msb = code >> shift;\n        add(msb);\n        code -= msb << shift;\n        length -= space;\n        space = 0;\n      }\n\n      if (space === 0) {\n        space = 8;\n      }\n    }\n  }\n\n  if (space !== 8) {\n    add(this.codes[256] >> this.lengths[256] - space);\n  }\n\n  return new Buffer(result);\n};\n\nHuffmanTable.prototype.decode = function decode(buffer) {\n  var result = [];\n  var subtree = this.tree;\n\n  for (var i = 0; i < buffer.length; i++) {\n    var byte = buffer[i];\n\n    for (var j = 0; j < 8; j++) {\n      var bit = byte & 128 ? 1 : 0;\n      byte = byte << 1;\n      subtree = subtree[bit];\n\n      if (subtree.length === 1) {\n        result.push(subtree[0]);\n        subtree = this.tree;\n      }\n    }\n  }\n\n  return new Buffer(result);\n}; // The initializer arrays for the Huffman tables are generated with feeding the tables from the\n// spec to this sed command:\n//\n//     sed -e \"s/^.* [|]//g\" -e \"s/|//g\" -e \"s/ .*//g\" -e \"s/^/  '/g\" -e \"s/$/',/g\"\n\n\nHuffmanTable.huffmanTable = new HuffmanTable(['1111111111000', '11111111111111111011000', '1111111111111111111111100010', '1111111111111111111111100011', '1111111111111111111111100100', '1111111111111111111111100101', '1111111111111111111111100110', '1111111111111111111111100111', '1111111111111111111111101000', '111111111111111111101010', '111111111111111111111111111100', '1111111111111111111111101001', '1111111111111111111111101010', '111111111111111111111111111101', '1111111111111111111111101011', '1111111111111111111111101100', '1111111111111111111111101101', '1111111111111111111111101110', '1111111111111111111111101111', '1111111111111111111111110000', '1111111111111111111111110001', '1111111111111111111111110010', '111111111111111111111111111110', '1111111111111111111111110011', '1111111111111111111111110100', '1111111111111111111111110101', '1111111111111111111111110110', '1111111111111111111111110111', '1111111111111111111111111000', '1111111111111111111111111001', '1111111111111111111111111010', '1111111111111111111111111011', '010100', '1111111000', '1111111001', '111111111010', '1111111111001', '010101', '11111000', '11111111010', '1111111010', '1111111011', '11111001', '11111111011', '11111010', '010110', '010111', '011000', '00000', '00001', '00010', '011001', '011010', '011011', '011100', '011101', '011110', '011111', '1011100', '11111011', '111111111111100', '100000', '111111111011', '1111111100', '1111111111010', '100001', '1011101', '1011110', '1011111', '1100000', '1100001', '1100010', '1100011', '1100100', '1100101', '1100110', '1100111', '1101000', '1101001', '1101010', '1101011', '1101100', '1101101', '1101110', '1101111', '1110000', '1110001', '1110010', '11111100', '1110011', '11111101', '1111111111011', '1111111111111110000', '1111111111100', '11111111111100', '100010', '111111111111101', '00011', '100011', '00100', '100100', '00101', '100101', '100110', '100111', '00110', '1110100', '1110101', '101000', '101001', '101010', '00111', '101011', '1110110', '101100', '01000', '01001', '101101', '1110111', '1111000', '1111001', '1111010', '1111011', '111111111111110', '11111111100', '11111111111101', '1111111111101', '1111111111111111111111111100', '11111111111111100110', '1111111111111111010010', '11111111111111100111', '11111111111111101000', '1111111111111111010011', '1111111111111111010100', '1111111111111111010101', '11111111111111111011001', '1111111111111111010110', '11111111111111111011010', '11111111111111111011011', '11111111111111111011100', '11111111111111111011101', '11111111111111111011110', '111111111111111111101011', '11111111111111111011111', '111111111111111111101100', '111111111111111111101101', '1111111111111111010111', '11111111111111111100000', '111111111111111111101110', '11111111111111111100001', '11111111111111111100010', '11111111111111111100011', '11111111111111111100100', '111111111111111011100', '1111111111111111011000', '11111111111111111100101', '1111111111111111011001', '11111111111111111100110', '11111111111111111100111', '111111111111111111101111', '1111111111111111011010', '111111111111111011101', '11111111111111101001', '1111111111111111011011', '1111111111111111011100', '11111111111111111101000', '11111111111111111101001', '111111111111111011110', '11111111111111111101010', '1111111111111111011101', '1111111111111111011110', '111111111111111111110000', '111111111111111011111', '1111111111111111011111', '11111111111111111101011', '11111111111111111101100', '111111111111111100000', '111111111111111100001', '1111111111111111100000', '111111111111111100010', '11111111111111111101101', '1111111111111111100001', '11111111111111111101110', '11111111111111111101111', '11111111111111101010', '1111111111111111100010', '1111111111111111100011', '1111111111111111100100', '11111111111111111110000', '1111111111111111100101', '1111111111111111100110', '11111111111111111110001', '11111111111111111111100000', '11111111111111111111100001', '11111111111111101011', '1111111111111110001', '1111111111111111100111', '11111111111111111110010', '1111111111111111101000', '1111111111111111111101100', '11111111111111111111100010', '11111111111111111111100011', '11111111111111111111100100', '111111111111111111111011110', '111111111111111111111011111', '11111111111111111111100101', '111111111111111111110001', '1111111111111111111101101', '1111111111111110010', '111111111111111100011', '11111111111111111111100110', '111111111111111111111100000', '111111111111111111111100001', '11111111111111111111100111', '111111111111111111111100010', '111111111111111111110010', '111111111111111100100', '111111111111111100101', '11111111111111111111101000', '11111111111111111111101001', '1111111111111111111111111101', '111111111111111111111100011', '111111111111111111111100100', '111111111111111111111100101', '11111111111111101100', '111111111111111111110011', '11111111111111101101', '111111111111111100110', '1111111111111111101001', '111111111111111100111', '111111111111111101000', '11111111111111111110011', '1111111111111111101010', '1111111111111111101011', '1111111111111111111101110', '1111111111111111111101111', '111111111111111111110100', '111111111111111111110101', '11111111111111111111101010', '11111111111111111110100', '11111111111111111111101011', '111111111111111111111100110', '11111111111111111111101100', '11111111111111111111101101', '111111111111111111111100111', '111111111111111111111101000', '111111111111111111111101001', '111111111111111111111101010', '111111111111111111111101011', '1111111111111111111111111110', '111111111111111111111101100', '111111111111111111111101101', '111111111111111111111101110', '111111111111111111111101111', '111111111111111111111110000', '11111111111111111111101110', '111111111111111111111111111111']); // ### String literal representation ###\n//\n// Literal **strings** can represent header names or header values. There's two variant of the\n// string encoding:\n//\n// String literal with Huffman encoding:\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 1 |  Value Length Prefix (7)  |\n//     +---+---+---+---+---+---+---+---+\n//     |   Value Length (0-N bytes)    |\n//     +---+---+---+---+---+---+---+---+\n//     ...\n//     +---+---+---+---+---+---+---+---+\n//     | Huffman Encoded Data  |Padding|\n//     +---+---+---+---+---+---+---+---+\n//\n// String literal without Huffman encoding:\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 0 |  Value Length Prefix (7)  |\n//     +---+---+---+---+---+---+---+---+\n//     |   Value Length (0-N bytes)    |\n//     +---+---+---+---+---+---+---+---+\n//     ...\n//     +---+---+---+---+---+---+---+---+\n//     |  Field Bytes Without Encoding |\n//     +---+---+---+---+---+---+---+---+\n\nHeaderSetCompressor.string = function writeString(str) {\n  str = new Buffer(str, 'utf8');\n  var huffman = HuffmanTable.huffmanTable.encode(str);\n\n  if (huffman.length < str.length) {\n    var length = HeaderSetCompressor.integer(huffman.length, 7);\n    length[0][0] |= 128;\n    return length.concat(huffman);\n  } else {\n    length = HeaderSetCompressor.integer(str.length, 7);\n    return length.concat(str);\n  }\n};\n\nHeaderSetDecompressor.string = function readString(buffer) {\n  var huffman = buffer[buffer.cursor] & 128;\n  var length = HeaderSetDecompressor.integer(buffer, 7);\n  var encoded = buffer.slice(buffer.cursor, buffer.cursor + length);\n  buffer.cursor += length;\n  return (huffman ? HuffmanTable.huffmanTable.decode(encoded) : encoded).toString('utf8');\n}; // ### Header represenations ###\n// The JavaScript object representation is described near the\n// `HeaderSetDecompressor.prototype._execute()` method definition.\n//\n// **All binary header representations** start with a prefix signaling the representation type and\n// an index represented using prefix coded integers:\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 1 |        Index (7+)         |  Indexed Representation\n//     +---+---------------------------+\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 0 | 1 |      Index (6+)       |\n//     +---+---+---+-------------------+  Literal w/ Indexing\n//     |       Value Length (8+)       |\n//     +-------------------------------+  w/ Indexed Name\n//     | Value String (Length octets)  |\n//     +-------------------------------+\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 0 | 1 |           0           |\n//     +---+---+---+-------------------+\n//     |       Name Length (8+)        |\n//     +-------------------------------+  Literal w/ Indexing\n//     |  Name String (Length octets)  |\n//     +-------------------------------+  w/ New Name\n//     |       Value Length (8+)       |\n//     +-------------------------------+\n//     | Value String (Length octets)  |\n//     +-------------------------------+\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 0 | 0 | 0 | 0 |  Index (4+)   |\n//     +---+---+---+-------------------+  Literal w/o Incremental Indexing\n//     |       Value Length (8+)       |\n//     +-------------------------------+  w/ Indexed Name\n//     | Value String (Length octets)  |\n//     +-------------------------------+\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 0 | 0 | 0 | 0 |       0       |\n//     +---+---+---+-------------------+\n//     |       Name Length (8+)        |\n//     +-------------------------------+  Literal w/o Incremental Indexing\n//     |  Name String (Length octets)  |\n//     +-------------------------------+  w/ New Name\n//     |       Value Length (8+)       |\n//     +-------------------------------+\n//     | Value String (Length octets)  |\n//     +-------------------------------+\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 0 | 0 | 0 | 1 |  Index (4+)   |\n//     +---+---+---+-------------------+  Literal never indexed\n//     |       Value Length (8+)       |\n//     +-------------------------------+  w/ Indexed Name\n//     | Value String (Length octets)  |\n//     +-------------------------------+\n//\n//       0   1   2   3   4   5   6   7\n//     +---+---+---+---+---+---+---+---+\n//     | 0 | 0 | 0 | 1 |       0       |\n//     +---+---+---+-------------------+\n//     |       Name Length (8+)        |\n//     +-------------------------------+  Literal never indexed\n//     |  Name String (Length octets)  |\n//     +-------------------------------+  w/ New Name\n//     |       Value Length (8+)       |\n//     +-------------------------------+\n//     | Value String (Length octets)  |\n//     +-------------------------------+\n//\n// The **Indexed Representation** consists of the 1-bit prefix and the Index that is represented as\n// a 7-bit prefix coded integer and nothing else.\n//\n// After the first bits, **all literal representations** specify the header name, either as a\n// pointer to the Header Table (Index) or a string literal. When the string literal representation\n// is used, the Index is set to 0 and the string literal starts at the second byte.\n//\n// For **all literal representations**, the specification of the header value comes next. It is\n// always represented as a string.\n\n\nvar representations = {\n  indexed: {\n    prefix: 7,\n    pattern: 0x80\n  },\n  literalIncremental: {\n    prefix: 6,\n    pattern: 0x40\n  },\n  contextUpdate: {\n    prefix: 0,\n    pattern: 0x20\n  },\n  literalNeverIndexed: {\n    prefix: 4,\n    pattern: 0x10\n  },\n  literal: {\n    prefix: 4,\n    pattern: 0x00\n  }\n};\n\nHeaderSetCompressor.header = function writeHeader(header) {\n  var representation,\n      buffers = [];\n\n  if (header.contextUpdate) {\n    representation = representations.contextUpdate;\n  } else if (typeof header.value === 'number') {\n    representation = representations.indexed;\n  } else if (header.index) {\n    representation = representations.literalIncremental;\n  } else if (header.mustNeverIndex) {\n    representation = representations.literalNeverIndexed;\n  } else {\n    representation = representations.literal;\n  }\n\n  if (representation === representations.contextUpdate) {\n    buffers.push(HeaderSetCompressor.integer(header.newMaxSize, 5));\n  } else if (representation === representations.indexed) {\n    buffers.push(HeaderSetCompressor.integer(header.value + 1, representation.prefix));\n  } else {\n    if (typeof header.name === 'number') {\n      buffers.push(HeaderSetCompressor.integer(header.name + 1, representation.prefix));\n    } else {\n      buffers.push(HeaderSetCompressor.integer(0, representation.prefix));\n      buffers.push(HeaderSetCompressor.string(header.name));\n    }\n\n    buffers.push(HeaderSetCompressor.string(header.value));\n  }\n\n  buffers[0][0][0] |= representation.pattern;\n  return Array.prototype.concat.apply([], buffers); // array of arrays of buffers -> array of buffers\n};\n\nHeaderSetDecompressor.header = function readHeader(buffer) {\n  var representation,\n      header = {};\n  var firstByte = buffer[buffer.cursor];\n\n  if (firstByte & 0x80) {\n    representation = representations.indexed;\n  } else if (firstByte & 0x40) {\n    representation = representations.literalIncremental;\n  } else if (firstByte & 0x20) {\n    representation = representations.contextUpdate;\n  } else if (firstByte & 0x10) {\n    representation = representations.literalNeverIndexed;\n  } else {\n    representation = representations.literal;\n  }\n\n  header.value = header.name = -1;\n  header.index = false;\n  header.contextUpdate = false;\n  header.newMaxSize = 0;\n  header.mustNeverIndex = false;\n\n  if (representation === representations.contextUpdate) {\n    header.contextUpdate = true;\n    header.newMaxSize = HeaderSetDecompressor.integer(buffer, 5);\n  } else if (representation === representations.indexed) {\n    header.value = header.name = HeaderSetDecompressor.integer(buffer, representation.prefix) - 1;\n  } else {\n    header.name = HeaderSetDecompressor.integer(buffer, representation.prefix) - 1;\n\n    if (header.name === -1) {\n      header.name = HeaderSetDecompressor.string(buffer);\n    }\n\n    header.value = HeaderSetDecompressor.string(buffer);\n    header.index = representation === representations.literalIncremental;\n    header.mustNeverIndex = representation === representations.literalNeverIndexed;\n  }\n\n  return header;\n}; // Integration with HTTP/2\n// =======================\n// This section describes the interaction between the compressor/decompressor and the rest of the\n// HTTP/2 implementation. The `Compressor` and the `Decompressor` makes up a layer between the\n// [framer](framer.html) and the [connection handling component](connection.html). They let most\n// frames pass through, except HEADERS and PUSH_PROMISE frames. They convert the frames between\n// these two representations:\n//\n//     {                                   {\n//      type: 'HEADERS',                    type: 'HEADERS',\n//      flags: {},                          flags: {},\n//      stream: 1,               <===>      stream: 1,\n//      headers: {                          data: Buffer\n//       N1: 'V1',                         }\n//       N2: ['V1', 'V2', ...],\n//       // ...\n//      }\n//     }\n//\n// There are possibly several binary frame that belong to a single non-binary frame.\n\n\nvar MAX_HTTP_PAYLOAD_SIZE = 16384; // The Compressor class\n// --------------------\n// The Compressor transform stream is basically stateless.\n\nutil.inherits(Compressor, TransformStream);\n\nfunction Compressor(log, type) {\n  TransformStream.call(this, {\n    objectMode: true\n  });\n  this._log = log.child({\n    component: 'compressor'\n  });\n  assert(type === 'REQUEST' || type === 'RESPONSE');\n  this._table = new HeaderTable(this._log);\n  this.tableSizeChangePending = false;\n  this.lowestTableSizePending = 0;\n  this.tableSizeSetting = DEFAULT_HEADER_TABLE_LIMIT;\n} // Changing the header table size\n\n\nCompressor.prototype.setTableSizeLimit = function setTableSizeLimit(size) {\n  this._table.setSizeLimit(size);\n\n  if (!this.tableSizeChangePending || size < this.lowestTableSizePending) {\n    this.lowestTableSizePending = size;\n  }\n\n  this.tableSizeSetting = size;\n  this.tableSizeChangePending = true;\n}; // `compress` takes a header set, and compresses it using a new `HeaderSetCompressor` stream\n// instance. This means that from now on, the advantages of streaming header encoding are lost,\n// but the API becomes simpler.\n\n\nCompressor.prototype.compress = function compress(headers) {\n  var compressor = new HeaderSetCompressor(this._log, this._table);\n\n  if (this.tableSizeChangePending) {\n    if (this.lowestTableSizePending < this.tableSizeSetting) {\n      compressor.send({\n        contextUpdate: true,\n        newMaxSize: this.lowestTableSizePending,\n        name: \"\",\n        value: \"\",\n        index: 0\n      });\n    }\n\n    compressor.send({\n      contextUpdate: true,\n      newMaxSize: this.tableSizeSetting,\n      name: \"\",\n      value: \"\",\n      index: 0\n    });\n    this.tableSizeChangePending = false;\n  }\n\n  var colonHeaders = [];\n  var nonColonHeaders = []; // To ensure we send colon headers first\n\n  for (var name in headers) {\n    if (name.trim()[0] === ':') {\n      colonHeaders.push(name);\n    } else {\n      nonColonHeaders.push(name);\n    }\n  }\n\n  function compressHeader(name) {\n    var value = headers[name];\n    name = String(name).toLowerCase(); // * To allow for better compression efficiency, the Cookie header field MAY be split into\n    //   separate header fields, each with one or more cookie-pairs.\n\n    if (name == 'cookie') {\n      if (!(value instanceof Array)) {\n        value = [value];\n      }\n\n      value = Array.prototype.concat.apply([], value.map(function (cookie) {\n        return String(cookie).split(';').map(trim);\n      }));\n    }\n\n    if (value instanceof Array) {\n      for (var i = 0; i < value.length; i++) {\n        compressor.write([name, String(value[i])]);\n      }\n    } else {\n      compressor.write([name, String(value)]);\n    }\n  }\n\n  colonHeaders.forEach(compressHeader);\n  nonColonHeaders.forEach(compressHeader);\n  compressor.end();\n  var chunk,\n      chunks = [];\n\n  while (chunk = compressor.read()) {\n    chunks.push(chunk);\n  }\n\n  return concat(chunks);\n}; // When a `frame` arrives\n\n\nCompressor.prototype._transform = function _transform(frame, encoding, done) {\n  // * and it is a HEADERS or PUSH_PROMISE frame\n  //   * it generates a header block using the compress method\n  //   * cuts the header block into `chunks` that are not larger than `MAX_HTTP_PAYLOAD_SIZE`\n  //   * for each chunk, it pushes out a chunk frame that is identical to the original, except\n  //     the `data` property which holds the given chunk, the type of the frame which is always\n  //     CONTINUATION except for the first frame, and the END_HEADERS/END_PUSH_STREAM flag that\n  //     marks the last frame and the END_STREAM flag which is always false before the end\n  if (frame.type === 'HEADERS' || frame.type === 'PUSH_PROMISE') {\n    var buffer = this.compress(frame.headers); // This will result in CONTINUATIONs from a PUSH_PROMISE being 4 bytes shorter than they could\n    // be, but that's not the end of the world, and it prevents us from going over MAX_HTTP_PAYLOAD_SIZE\n    // on the initial PUSH_PROMISE frame.\n\n    var adjustment = frame.type === 'PUSH_PROMISE' ? 4 : 0;\n    var chunks = cut(buffer, MAX_HTTP_PAYLOAD_SIZE - adjustment);\n\n    for (var i = 0; i < chunks.length; i++) {\n      var chunkFrame;\n      var first = i === 0;\n      var last = i === chunks.length - 1;\n\n      if (first) {\n        chunkFrame = util._extend({}, frame);\n        chunkFrame.flags = util._extend({}, frame.flags);\n        chunkFrame.flags['END_' + frame.type] = last;\n      } else {\n        chunkFrame = {\n          type: 'CONTINUATION',\n          flags: {\n            END_HEADERS: last\n          },\n          stream: frame.stream\n        };\n      }\n\n      chunkFrame.data = chunks[i];\n      this.push(chunkFrame);\n    }\n  } // * otherwise, the frame is forwarded without taking any action\n  else {\n      this.push(frame);\n    }\n\n  done();\n}; // The Decompressor class\n// ----------------------\n// The Decompressor is a stateful transform stream, since it has to collect multiple frames first,\n// and the decoding comes after unifying the payload of those frames.\n//\n// If there's a frame in progress, `this._inProgress` is `true`. The frames are collected in\n// `this._frames`, and the type of the frame and the stream identifier is stored in `this._type`\n// and `this._stream` respectively.\n\n\nutil.inherits(Decompressor, TransformStream);\n\nfunction Decompressor(log, type) {\n  TransformStream.call(this, {\n    objectMode: true\n  });\n  this._log = log.child({\n    component: 'compressor'\n  });\n  assert(type === 'REQUEST' || type === 'RESPONSE');\n  this._table = new HeaderTable(this._log);\n  this._inProgress = false;\n  this._base = undefined;\n} // Changing the header table size\n\n\nDecompressor.prototype.setTableSizeLimit = function setTableSizeLimit(size) {\n  this._table.setSizeLimit(size);\n}; // `decompress` takes a full header block, and decompresses it using a new `HeaderSetDecompressor`\n// stream instance. This means that from now on, the advantages of streaming header decoding are\n// lost, but the API becomes simpler.\n\n\nDecompressor.prototype.decompress = function decompress(block) {\n  var decompressor = new HeaderSetDecompressor(this._log, this._table);\n  decompressor.end(block);\n  var seenNonColonHeader = false;\n  var headers = {};\n  var pair;\n\n  while (pair = decompressor.read()) {\n    var name = pair[0];\n    var value = pair[1];\n    var isColonHeader = name.trim()[0] === ':';\n\n    if (seenNonColonHeader && isColonHeader) {\n      this.emit('error', 'PROTOCOL_ERROR');\n      return headers;\n    }\n\n    seenNonColonHeader = !isColonHeader;\n\n    if (name in headers) {\n      if (headers[name] instanceof Array) {\n        headers[name].push(value);\n      } else {\n        headers[name] = [headers[name], value];\n      }\n    } else {\n      headers[name] = value;\n    }\n  } // * If there are multiple Cookie header fields after decompression, these MUST be concatenated\n  //   into a single octet string using the two octet delimiter of 0x3B, 0x20 (the ASCII\n  //   string \"; \").\n\n\n  if ('cookie' in headers && headers['cookie'] instanceof Array) {\n    headers['cookie'] = headers['cookie'].join('; ');\n  }\n\n  return headers;\n}; // When a `frame` arrives\n\n\nDecompressor.prototype._transform = function _transform(frame, encoding, done) {\n  // * and the collection process is already `_inProgress`, the frame is simply stored, except if\n  //   it's an illegal frame\n  if (this._inProgress) {\n    if (frame.type !== 'CONTINUATION' || frame.stream !== this._base.stream) {\n      this._log.error('A series of HEADER frames were not continuous');\n\n      this.emit('error', 'PROTOCOL_ERROR');\n      return;\n    }\n\n    this._frames.push(frame);\n  } // * and the collection process is not `_inProgress`, but the new frame's type is HEADERS or\n  //   PUSH_PROMISE, a new collection process begins\n  else if (frame.type === 'HEADERS' || frame.type === 'PUSH_PROMISE') {\n      this._inProgress = true;\n      this._base = util._extend({}, frame);\n      this._frames = [frame];\n    } // * otherwise, the frame is forwarded without taking any action\n    else {\n        this.push(frame);\n      } // * When the frame signals that it's the last in the series, the header block chunks are\n  //   concatenated, the headers are decompressed, and a new frame gets pushed out with the\n  //   decompressed headers.\n\n\n  if (this._inProgress && (frame.flags.END_HEADERS || frame.flags.END_PUSH_PROMISE)) {\n    var buffer = concat(this._frames.map(function (frame) {\n      return frame.data;\n    }));\n\n    try {\n      var headers = this.decompress(buffer);\n    } catch (error) {\n      this._log.error({\n        err: error\n      }, 'Header decompression error');\n\n      this.emit('error', 'COMPRESSION_ERROR');\n      return;\n    }\n\n    this.push(util._extend(this._base, {\n      headers: headers\n    }));\n    this._inProgress = false;\n  }\n\n  done();\n}; // Helper functions\n// ================\n// Concatenate an array of buffers into a new buffer\n\n\nfunction concat(buffers) {\n  var size = 0;\n\n  for (var i = 0; i < buffers.length; i++) {\n    size += buffers[i].length;\n  }\n\n  var concatenated = new Buffer(size);\n\n  for (var cursor = 0, j = 0; j < buffers.length; cursor += buffers[j].length, j++) {\n    buffers[j].copy(concatenated, cursor);\n  }\n\n  return concatenated;\n} // Cut `buffer` into chunks not larger than `size`\n\n\nfunction cut(buffer, size) {\n  var chunks = [];\n  var cursor = 0;\n\n  do {\n    var chunkSize = Math.min(size, buffer.length - cursor);\n    chunks.push(buffer.slice(cursor, cursor + chunkSize));\n    cursor += chunkSize;\n  } while (cursor < buffer.length);\n\n  return chunks;\n}\n\nfunction trim(string) {\n  return string.trim();\n}","map":null,"metadata":{},"sourceType":"script"}